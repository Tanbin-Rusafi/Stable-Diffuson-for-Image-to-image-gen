{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Used pix2pix dataset. I had to resize images to 32 for making it compatible to the unconditional unet that I have used. Used 'CompVis/stable-diffusion-v1-3' as pre trained model to fine tune. But it's unet requires text embeddings as well, so instead I used 'google/ddpm-cifar10-32' as an unconditional unet.","metadata":{}},{"cell_type":"code","source":"!pip install diffusers transformers datasets accelerate torch torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:45:55.803175Z","iopub.execute_input":"2024-12-05T17:45:55.803860Z","iopub.status.idle":"2024-12-05T17:46:07.106856Z","shell.execute_reply.started":"2024-12-05T17:45:55.803824Z","shell.execute_reply":"2024-12-05T17:46:07.106017Z"}},"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.26.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.6.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.31.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport os, time, pickle, json \nfrom glob import glob \nfrom PIL import Image \nimport cv2 \nfrom typing import List, Tuple, Dict \nfrom statistics import mean \nfrom tqdm import tqdm \n\nimport torch \nimport torch.nn as nn \nfrom torchvision import transforms \nfrom torchvision.utils import save_image \nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\n\n# Set device to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:46:49.000472Z","iopub.execute_input":"2024-12-05T17:46:49.000833Z","iopub.status.idle":"2024-12-05T17:46:54.424708Z","shell.execute_reply.started":"2024-12-05T17:46:49.000802Z","shell.execute_reply":"2024-12-05T17:46:54.423791Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_generated_token_here' with the token you generated\nlogin(token=\"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\")\n\nprint(\"Successfully logged in to Hugging Face!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:46:58.594208Z","iopub.execute_input":"2024-12-05T17:46:58.595240Z","iopub.status.idle":"2024-12-05T17:46:58.813070Z","shell.execute_reply.started":"2024-12-05T17:46:58.595202Z","shell.execute_reply":"2024-12-05T17:46:58.812119Z"}},"outputs":[{"name":"stdout","text":"Successfully logged in to Hugging Face!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Dataset**","metadata":{}},{"cell_type":"code","source":"MEAN = (0.5, 0.5, 0.5,)\nSTD = (0.5, 0.5, 0.5,)\nRESIZE = 32\n\n\ndef read_path(filepath) -> List[str]:\n    root_path = \"../input/pix2pix-dataset/maps/maps\"\n    path = os.path.join(root_path, filepath)\n    dataset = []\n    for p in glob(path+\"/\"+\"*.jpg\"):\n        dataset.append(p)\n    return dataset \n\n\nclass Transform():\n    def __init__(self, resize=RESIZE, mean=MEAN, std=STD):\n        self.data_transform = transforms.Compose([\n            transforms.Resize((resize, resize)), \n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        \n    def __call__(self, img: Image.Image):\n        return self.data_transform(img)\n\n    \nclass Dataset(object):\n    def __init__(self, files: List[str]):\n        self.files = files \n        self.trasformer = Transform()\n        \n    def _separate(self, img) -> Tuple[Image.Image, Image.Image]:\n        img = np.array(img, dtype=np.uint8)\n        h, w, _ = img.shape\n        w = int(w/2)\n        return Image.fromarray(img[:, w:, :]), Image.fromarray(img[:, :w, :])\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        img = Image.open(self.files[idx])\n        input, output = self._separate(img)\n        input_tensor = self.trasformer(input)\n        output_tensor = self.trasformer(output)\n        return input_tensor, output_tensor \n    \n    def __len__(self):\n        return len(self.files)\n    \ndef show_img_sample(img: torch.Tensor, img1: torch.Tensor):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n    ax = axes.ravel()\n    ax[0].imshow(img.permute(1, 2, 0))\n    ax[0].set_xticks([])\n    ax[0].set_yticks([])\n    ax[0].set_title(\"input image\", c=\"g\")\n    ax[1].imshow(img1.permute(1, 2, 0))\n    ax[1].set_xticks([])\n    ax[1].set_yticks([])\n    ax[1].set_title(\"label image\", c=\"g\")\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:47:06.909924Z","iopub.execute_input":"2024-12-05T17:47:06.910258Z","iopub.status.idle":"2024-12-05T17:47:06.922281Z","shell.execute_reply.started":"2024-12-05T17:47:06.910228Z","shell.execute_reply":"2024-12-05T17:47:06.921493Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train = read_path(\"train\")\nval = read_path(\"val\")\ntrain_ds = Dataset(train)\nval_ds = Dataset(val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:47:10.164014Z","iopub.execute_input":"2024-12-05T17:47:10.164350Z","iopub.status.idle":"2024-12-05T17:47:10.210082Z","shell.execute_reply.started":"2024-12-05T17:47:10.164321Z","shell.execute_reply":"2024-12-05T17:47:10.209410Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"show_img_sample(train_ds.__getitem__(1)[0], train_ds.__getitem__(1)[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:47:11.498996Z","iopub.execute_input":"2024-12-05T17:47:11.499734Z","iopub.status.idle":"2024-12-05T17:47:11.903909Z","shell.execute_reply.started":"2024-12-05T17:47:11.499699Z","shell.execute_reply":"2024-12-05T17:47:11.903317Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x800 with 2 Axes>","image/png":"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"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"def validate_model():\n    unet.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    with torch.no_grad():  # No need to calculate gradients during validation\n        for batch in val_dataloader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(accelerator.device), targets.to(accelerator.device)\n            latents = inputs  \n            noise = torch.randn(latents.shape).to(latents.device)\n            timesteps = torch.randint(\n                0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device\n            ).long()\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n            noise_pred = unet(noisy_latents, timesteps, None).sample\n            loss = F.mse_loss(noise_pred, noise)\n            val_loss += loss.item()\n\n    return val_loss / len(val_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:47:42.100958Z","iopub.execute_input":"2024-12-05T17:47:42.101649Z","iopub.status.idle":"2024-12-05T17:47:42.107300Z","shell.execute_reply.started":"2024-12-05T17:47:42.101619Z","shell.execute_reply":"2024-12-05T17:47:42.106392Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DModel\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\n\n# Configuration dictionary\nconfig = {\n    \"train_batch_size\": 1,\n    \"val_batch_size\": 1,\n    \"learning_rate\": 1e-4,\n    \"gradient_accumulation_steps\": 2,\n    \"max_train_steps\": 43840,\n    \"max_grad_norm\": 1.0,\n    \"seed\": 42,\n    \"output_dir\": \"/kaggle/working/\",\n    \"mixed_precision\": \"fp16\",  # Mixed precision for memory optimization\n    \"gradient_checkpointing\": True,  # Gradient checkpointing for memory optimization\n}\n\n# Seed for reproducibility\ntorch.manual_seed(config[\"seed\"])\n\n# Initialize the pipeline and models\npipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-3\")\nvae = pipeline.vae\n#unet = pipeline.unet\nunet = UNet2DModel.from_pretrained(\"google/ddpm-cifar10-32\")\n\n# Enable memory optimizations\n#unet.enable_gradient_checkpointing()\n\n# Optimizer\noptimizer = torch.optim.AdamW(unet.parameters(), lr=config[\"learning_rate\"])\n\n# Noise scheduler\nnoise_scheduler = DDPMScheduler(\n    beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", num_train_timesteps=1000\n)\n\n# Accelerator for multi-GPU and mixed precision\naccelerator = Accelerator(mixed_precision=config[\"mixed_precision\"])\n\n# Prepare datasets and dataloaders\ntrain_dataloader = DataLoader(train_ds, batch_size=config[\"train_batch_size\"], shuffle=True)\nval_dataloader = DataLoader(val_ds, batch_size=config[\"val_batch_size\"], shuffle=False)\n\n# Prepare models and dataloaders for accelerator\nunet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n\n# Move VAE to the accelerator's device\nvae.to(accelerator.device)\n\n# Training loop\nprogress_bar = tqdm(range(config[\"max_train_steps\"]))\nprogress_bar.set_description(\"Training Steps\")\nglobal_step = 0\n\n# Define number of epochs\nnum_epochs = config[\"max_train_steps\"] // len(train_dataloader)\n\n# Initialize loss tracking\ntraining_loss = 0.0\n\nfor epoch in range(num_epochs):\n    # Set model to training mode at the start of each epoch\n    unet.train()\n    \n    for step, batch in enumerate(train_dataloader):\n        if global_step >= config[\"max_train_steps\"]:\n            break\n    \n        with accelerator.accumulate(unet):\n            # Split input and target images\n            inputs, targets = batch\n            inputs, targets = inputs.to(accelerator.device), targets.to(accelerator.device)\n    \n            # Directing input images into latent space\n            latents = inputs  \n    \n            # Sample random noise and timesteps\n            noise = torch.randn(latents.shape).to(latents.device)\n            timesteps = torch.randint(\n                0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device\n            ).long()\n    \n            # Add noise to latents\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n    \n            # Predict noise residual\n            noise_pred = unet(noisy_latents, timesteps, None).sample\n    \n            # Compute MSE loss\n            loss = F.mse_loss(noise_pred, noise)\n    \n            # Backpropagation\n            accelerator.backward(loss)\n            if accelerator.sync_gradients:\n                accelerator.clip_grad_norm_(unet.parameters(), config[\"max_grad_norm\"])\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Log the training loss\n            training_loss += loss.detach().item()\n    \n        # Update progress bar and global step\n        progress_bar.set_postfix({\"Training Loss\": training_loss / (step + 1)})\n        progress_bar.update(1)\n        global_step += 1\n\n    print(f\"Epoch {epoch + 1} - Training Loss: {training_loss / len(train_dataloader)}\")\n    # At the end of the epoch, validate the model\n    validation_loss = validate_model()\n    print(f\"Epoch {epoch + 1} - Validation Loss: {validation_loss}\")\n\n# Save the trained model\nif accelerator.is_main_process:\n    # Save the UNet model\n    unet.save_pretrained(config[\"output_dir\"])\n\n    # Save the noise scheduler configuration\n    noise_scheduler.save_pretrained(config[\"output_dir\"])\n\nprint(f\"Model saved to {config['output_dir']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For understanding configurations & Unet architecture\n\nprint(\"Inputs shape:\", inputs.shape)\nprint(\"Latents shape:\", latents.shape)\nprint(\"Noisy latents shape:\", noisy_latents.shape)\n\nprint(unet.config)\n\nprint(\"Timesteps:\", timesteps.shape, timesteps.dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:59:28.090267Z","iopub.execute_input":"2024-12-05T16:59:28.091178Z","iopub.status.idle":"2024-12-05T16:59:28.096492Z","shell.execute_reply.started":"2024-12-05T16:59:28.091145Z","shell.execute_reply":"2024-12-05T16:59:28.095482Z"}},"outputs":[{"name":"stdout","text":"Inputs shape: torch.Size([1, 3, 32, 32])\nLatents shape: torch.Size([1, 4, 4, 4])\nNoisy latents shape: torch.Size([1, 4, 4, 4])\nFrozenDict([('sample_size', 64), ('in_channels', 4), ('out_channels', 4), ('center_input_sample', False), ('flip_sin_to_cos', True), ('freq_shift', 0), ('down_block_types', ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D']), ('mid_block_type', 'UNetMidBlock2DCrossAttn'), ('up_block_types', ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D']), ('only_cross_attention', False), ('block_out_channels', [320, 640, 1280, 1280]), ('layers_per_block', 2), ('downsample_padding', 1), ('mid_block_scale_factor', 1), ('dropout', 0.0), ('act_fn', 'silu'), ('norm_num_groups', 32), ('norm_eps', 1e-05), ('cross_attention_dim', 768), ('transformer_layers_per_block', 1), ('reverse_transformer_layers_per_block', None), ('encoder_hid_dim', None), ('encoder_hid_dim_type', None), ('attention_head_dim', 8), ('num_attention_heads', None), ('dual_cross_attention', False), ('use_linear_projection', False), ('class_embed_type', None), ('addition_embed_type', None), ('addition_time_embed_dim', None), ('num_class_embeds', None), ('upcast_attention', False), ('resnet_time_scale_shift', 'default'), ('resnet_skip_time_act', False), ('resnet_out_scale_factor', 1.0), ('time_embedding_type', 'positional'), ('time_embedding_dim', None), ('time_embedding_act_fn', None), ('timestep_post_act', None), ('time_cond_proj_dim', None), ('conv_in_kernel', 3), ('conv_out_kernel', 3), ('projection_class_embeddings_input_dim', None), ('attention_type', 'default'), ('class_embeddings_concat', False), ('mid_block_only_cross_attention', None), ('cross_attention_norm', None), ('addition_embed_type_num_heads', 64), ('_use_default_values', ['time_embedding_dim', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'attention_type', 'only_cross_attention', 'resnet_time_scale_shift', 'mid_block_type', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'encoder_hid_dim', 'timestep_post_act', 'time_embedding_act_fn', 'conv_out_kernel', 'num_class_embeds', 'reverse_transformer_layers_per_block', 'addition_time_embed_dim', 'addition_embed_type', 'dual_cross_attention', 'class_embed_type', 'time_embedding_type', 'use_linear_projection', 'encoder_hid_dim_type', 'projection_class_embeddings_input_dim', 'upcast_attention', 'conv_in_kernel', 'cross_attention_norm', 'class_embeddings_concat', 'num_attention_heads', 'resnet_out_scale_factor', 'dropout', 'time_cond_proj_dim']), ('_class_name', 'UNet2DConditionModel'), ('_diffusers_version', '0.2.2'), ('_name_or_path', '/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-3/snapshots/159f26fccae6e3ea26c797ad24c5e7033da5b1ee/unet')])\nTimesteps: torch.Size([1]) torch.int64\n","output_type":"stream"}],"execution_count":11}]}